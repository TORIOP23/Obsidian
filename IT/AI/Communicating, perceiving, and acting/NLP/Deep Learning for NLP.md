## Word Embeddings 
- Representing words as points in a hight-dimensional space, rather than as atomic value
## RNN for NLP
- Use of recurrent neutral networks to capture meaning and long-distance context as text is processed sequentially
## Sequence to Sequence Model
## The Transformer Architecture

## Pretraining and Transfer Learning

## State of the art




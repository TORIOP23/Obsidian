- Attention is all you need - 2017
![[transformer.png]]
## Encoder
### [[Word Embeddings]]
### Positional Encoding
- một từ ở vị trí khác nhau của câu lại mang ý nghĩa khác nhau
### Self-Attention
- cơ chế giúp Transformers "hiểu" được sự liên quan giữa các từ trong một câu. Ví dụ như từ "kicked" trong câu "I kicked the ball" (tôi đã đá quả bóng) liên quan như thế nào đến các từ khác?
### Multi-head Attention
### Residuals
### Feed Forward
## Decoder
### Masked Multi-head Attention
### Quá trình decode

# Variant
- [[BERT]] - only encoder
- BART
- DPR bi-encoders
- DistilBERT
- ELECTRA
- MobileBERT
- RoBERTa
- RetriBERT
- MPNet
- SentenceTransformers bi-encoders with the above transformer architectures

# Reference 
- [Viblo](https://viblo.asia/p/transformers-nguoi-may-bien-hinh-bien-doi-the-gioi-nlp-924lJPOXKPM)
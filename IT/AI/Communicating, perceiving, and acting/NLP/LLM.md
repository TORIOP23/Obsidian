- Large Language Model
3 kind of LLM 
- Generic or raw language models
- Instruction-tuned language models
- Dialog-tuned language models
## Recurrent layer
- Interprets the words in the input text in sequence. It captures the relationship between words in a sentence.
## Feedforward layers (FFN)
- Made of up multiple fully connected layers that transform the input embeddings. In so doing, these layers enable the model to glean higher-level abstractions — that is, to understand the user's intent with the text input.
## Embedding layer
- creates embeddings from the input text
## Attention layers
- Enables a language model to focus on single parts of the input text that is relevant to the task at hand
## [[Transformer model]]

## [[Prompt engineering]]
## [[Fine-tuning]]
## [[RAG]]


# Reference 
- [LLM](https://www.elastic.co/what-is/large-language-models)
- 
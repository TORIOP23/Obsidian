- At each stage, the attention layers can access all the words in the initial sentence.
- These models are often characterized as having “bi-directional” attention, and are often called _auto-encoding models_.
- Tasks: Sentence classification, named entity recognition, extractive question answering
- **Train**: **masked language modelling** (MLM) approach
Example
- **ALBERT** - A Lite BERT for Self-supervised Learning of Language Representations
- BERT
- DistilBERT
- ELECTRA
- **RoBERTa** - Robustly Optimized BERT Pretraining Approach
- Representing words as points in a hight-dimensional space, rather than as atomic value
# Context independent
### [[Word2vec]]
### [[GloVe]] (Global Vectors)
### [[FastText]] - Facebook
# Context dependent
## RNN based
### [[ELMo]] (Embeddings from Language Models)
### [[Cove]]
## Transformer based
### [[Encoder models |BERT like]]




## Reduce dimension
- [[SVD]]
- PCA
- Auto encoder
- 
# Reference
- [Spot Intelligence](https://spotintelligence.com/2024/10/01/text-representation-a-simple-explanation-of-complex-techniques/)
- [phamdinhkhanh](https://phamdinhkhanh.github.io/2019/04/29/ModelWord2Vec.html)
- [NLPlanet](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings)